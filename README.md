# product-images-captioning

Данный репозиторий содержит код моделей, которые можно использовать для генерации описаний товаров магазинов одежды.

## Зависимости
* Python 3.10.X
* torch==1.13.0 (GPU/CPU)
* `pip install requirements.txt`

## Запуск
* Обучение + валидация:
`python src/train.py`, файл с конфигурацией — <i>configs/train.yaml</i>.
* Тестирование:
`python src/test.py`, файл с конфигурацией — <i>configs/test.yaml</i>.
* Предсказание на одном изображении:
`python src/inference.py`, файл с конфигурацией — <i>configs/inference.yaml</i>.
* Создание обработанных датасетов из исходного:
```
Usage: create_dataset.py [OPTIONS]
Options:
  --input-dataset-filepath TEXT  [required]
  --output-dir TEXT              [required]
  --image-dir TEXT               [required]
  --val-size FLOAT
  --test-size FLOAT
  --seed INTEGER
```

## Данные

Для обучения и оценки качества модели используются данные из соревнования 
[H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations).

Из датасета были удалены записи с дублирующимися описаниями и записи, не имеющие соответствующих картинок.

Для проведения экспериментов данные были разбиты на три части: <i>train (80%)</i>,
<i>val (10%)</i>, <i>test (10%)</i>. <i>Train</i> использовалась для обучения моделей, <i>val</i> — для подбора
гиперпараметров, на <i>test</i> проводилась финальная оценка качества.

### Извлечение признаков

Исходные изображения приводились к размеру 224x224 и нормализовывались с использованием следующих статистик:
`'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5],`.

Токенизация текста производилась при помощи tokenizer'а используемого decoder'а (каждый токен сопоставлялся с целым числом
от 0 до <i>N - 1</i>, где <i>N</i> — мощность словаря).
К словарь были добавлены специальные токены: `'[PAD]'` — для уравнивания длин последовательностей,
`'[BOS]'` — для обозначения начал последовательностей, `'[EOS]'` — для обозначения окончаний последовательностей.

## Модель

Для решения задачи используется архитектура Encoder-Decoder.
В качестве encoder'а используется [vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224),
в качестве decoder'а [distilgpt2](https://huggingface.co/distilgpt2).
На вход модели передавались признаки, извлеченные из изображений. На выходе получалась последовательность идентификаторов токенов,
которые переводились в текст.

## Метрика

Для оценки качества работы модели была использована метрика [SacreBLEU](https://github.com/mjpost/sacrebleu) 
(используется [данная реализация](https://huggingface.co/spaces/evaluate-metric/sacrebleu) с параметрами по умолчанию).
Метрика [BLEU](https://en.wikipedia.org/wiki/BLEU) была создана для оценки качества машинного перевода, но может быть применима
для любой задачи из семейства seq2seq.

## Результаты экспериметнов
...

## Примеры работы
...

## Ресурсы

Среднее время работы на одном изображении ~ 3.305s. Среднее потребление памяти ~ 4.2GB. Для замеров использовалась
функция `torch.profiler.profile(...)`. 

Измерения производилось на CPU (AMD Ryzen 7 5700U).
